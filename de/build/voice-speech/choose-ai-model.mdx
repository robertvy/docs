---
title: "KI-Modell auswählen"
description: "Wählen und konfiguriere das Sprachmodell, das die Intelligenz deines Agenten steuert"
icon: "brain"
keywords: "KI-Modell, GPT-4, Anthropic Claude, Sprachmodell, LLM, OpenAI, Azure OpenAI"
og:description: "Wähle das optimale KI-Sprachmodell für deinen Agent: GPT-4, Claude, Gemini und mehr."
---

## Übersicht

Das KI-Modell (LLM) ist das Gehirn deines Sprachagenten. Es verarbeitet, was Kunden sagen, versteht ihre Absicht, überlegt die beste Antwort und entscheidet, wann Aktionen ausgeführt werden sollen. Die Wahl des richtigen Modells bedeutet, Leistung, Latenz, Kosten und Compliance-Anforderungen in Einklang zu bringen.

<Note>
Die Modellauswahl erfolgt unter **Modelle > Modell** in deiner Agentenkonfiguration. Änderungen werden sofort wirksam – kein separater Veröffentlichungsschritt erforderlich.
</Note>

## Sprachmodelle verstehen

Sprachmodelle werden mit riesigen Mengen an Text trainiert, um menschliche Sprache zu verstehen und zu generieren. In Sprachagenten interpretiert das LLM Kundenanfragen, überlegt die beste Antwort basierend auf deinen Anweisungen und deiner Wissensdatenbank, entscheidet, wann Aktionen wie Weiterleitungen oder Buchungen verwendet werden, generiert natürliche Gesprächsantworten und behält den Kontext während des gesamten Gesprächs bei.

Verschiedene Modelle zeichnen sich bei verschiedenen Aufgaben aus. Einige priorisieren Geschwindigkeit, andere Genauigkeit, und einige bieten das beste Gleichgewicht für konversationelle KI.

---

## Empfohlene Modelle

Basierend auf der realen Leistung von Tausenden von Sprachagenten sind hier die bewährten Modelle für verschiedene Anwendungsfälle:

<AccordionGroup>
  <Accordion title="Am besten für die meisten Anwendungsfälle: GPT-4.1 Mini" icon="star" defaultOpen>
    **Unsere Top-Empfehlung** für produktive Sprachagenten.

    **Warum es funktioniert:**
    - Hervorragende Latenz (~700-800ms Antwortzeit)
    - 70%+ Erfolgsrate beim Funktionsaufruf (Weiterleitungen, Buchungen, Aktionen)
    - Starke Befolgung von Anweisungen
    - Erschwingliche Kosten

    **Verwendung für:**
    - Kundensupport
    - Terminbuchung
    - Auftragsabwicklung
    - Die meisten Gesprächsszenarien

    **Verfügbar bei:** OpenAI, Azure OpenAI (EU-gehostet)
  </Accordion>

  <Accordion title="Für komplexe Aufgaben: GPT-4.1" icon="brain">
    Wenn du maximale Intelligenz und Schlussfolgerungsfähigkeit benötigen.

    **Warum es funktioniert:**
    - Erstklassige Schlussfolgerung und mehrstufige Logik
    - Bewältigt komplexe Fehlerbehebung
    - Überlegenes Kontextverständnis

    **Kompromisse:**
    - Höhere Latenz als GPT-4.1 Mini
    - Höhere Kosten pro Gespräch

    **Verwendung für:**
    - Technischer Support mit komplexer Diagnose
    - Mehrstufige Verkaufsgespräche
    - Aufgaben, die tiefes Nachdenken erfordern

    **Verfügbar bei:** OpenAI, Azure OpenAI (EU-gehostet)
  </Accordion>

  <Accordion title="Schnell & erschwinglich: Claude Haiku 4.5" icon="bolt">
    Anthropics schnellstes Modell mit starker Leistung.

    **Warum es funktioniert:**
    - Antwortzeiten unter einer Sekunde
    - Gutes Gleichgewicht zwischen Geschwindigkeit und Intelligenz
    - Constitutional AI für sicherere Antworten
    - Niedrigere Kosten als Sonnet

    **Verwendung für:**
    - Hochvolumen-Callcenter
    - Geschwindigkeitskritische Anwendungen
    - Budgetbewusste Bereitstellungen

    **Verfügbar bei:** Anthropic
  </Accordion>

  <Accordion title="Ultra-schnell Open Source: Groq Llama 3.1 8B" icon="rocket">
    Schnellste verfügbare Option, angetrieben durch Groqs benutzerdefinierte Hardware.

    **Warum es funktioniert:**
    - Antwortzeiten unter 500ms
    - Verarbeitet Hunderte von Token pro Sekunde
    - Open-Source-Modell
    - Sehr niedrige Kosten

    **Kompromisse:**
    - Weniger intelligent als GPT-4.1 oder Claude
    - Gelegentliche Latenzspitzen unter Last
    - Besser für einfachere Gespräche

    **Verwendung für:**
    - Einfache Qualifizierungsanrufe
    - IVR und Routing
    - Hochvolumen-Szenarien mit geringer Komplexität

    **Verfügbar bei:** Groq
  </Accordion>

  <Accordion title="Mehr Intelligenz von Groq: Llama 3.3 70B" icon="layer-group">
    Bessere Schlussfolgerungsfähigkeit bei Beibehaltung des Geschwindigkeitsvorteils von Groq.

    **Warum es funktioniert:**
    - Qualitätssteigerung gegenüber dem 8B-Modell
    - Immer noch schnell auf Groq-Infrastruktur
    - Guter Mittelweg

    **Verwendung für:**
    - Wenn die Qualität von Llama 3.1 8B nicht ausreicht
    - Geschwindigkeit erforderlich, aber mehr Intelligenz

    **Verfügbar bei:** Groq
  </Accordion>
</AccordionGroup>

<Tip>
**Schnelle Entscheidungshilfe:**
- Starten du mit **GPT-4.1 Mini** → zuverlässig, schnell, großartig für die meisten Anwendungsfälle
- Mehr Schlussfolgerung benötigt? → **GPT-4.1**
- Schneller/günstiger benötigt? → **Claude Haiku 4.5**
- Schnellstes benötigt? → **Groq Llama 3.1 8B** (aber weniger intelligent)
</Tip>

---

## Modellauswahl-Oberfläche

### Anbieter-Katalog

Die Modellauswahl-Oberfläche gruppiert Anbieter mit hilfreichen Metadaten:

<CardGroup cols={2}>
  <Card title="Anbieter-Symbole" icon="building">
    Visuelles Branding für OpenAI, Anthropic, Groq, Azure und mehr
  </Card>
  <Card title="EU-gehostet-Badge" icon="location-dot">
    Zeigt Modelle an, die Daten in EU-Regionen verarbeiten
  </Card>
  <Card title="Modellanzahl" icon="list-ol">
    Zeigt, wie viele Modelle von jedem Anbieter verfügbar sind
  </Card>
  <Card title="Aktive Auswahl" icon="check">
    Hebt dein aktuell ausgewähltes Modell hervor
  </Card>
</CardGroup>

### Filtern und Suchen

Klicke auf einen Anbieter, um die Modelltabelle nur auf diesen Anbieter zu filtern. Verwende das Suchfeld, um schnell bestimmte Modelle nach Name oder Fähigkeit zu finden.

---

## Details zu Modellanbietern

### OpenAI

OpenAI-Modelle bieten das beste Gleichgewicht zwischen Zuverlässigkeit und Funktionsaufruf für Sprachagenten.

**GPT-4.1 Mini** ⭐ **Empfohlen**
- **Reale Leistung:** ~700-800ms Antwortzeit, 70%+ Erfolgsrate beim Funktionsaufruf
- **Am besten für:** Produktive Sprachagenten - Support, Buchung, Vertrieb
- **Warum es funktioniert:** Bewährte Zuverlässigkeit, hervorragende Tool-Nutzung, gute Latenz

**GPT-4.1**
- **Reale Leistung:** Höhere Latenz als Mini, aber überlegene Schlussfolgerung
- **Am besten für:** Komplexe mehrstufige Gespräche, technischer Support
- **Kompromiss:** Höhere Kosten und Latenz für mehr Intelligenz

**GPT-5 Serie** (Mini, Nano)
- **Status:** Modelle der nächsten Generation mit fortgeschrittenem Denkvermögen
- **Überlegungen:** GPT-5 hat höhere Latenz (~1s+); GPT-5 Mini bietet besseres Gleichgewicht
- **Am besten für:** Aufgaben, bei denen Intelligenz wichtiger als Geschwindigkeit ist

**Legacy-Modelle** (GPT-4o, GPT-4o Mini)
- **Status:** Immer noch funktional, aber erwägen du die GPT-4.1/5-Serie für neue Agenten

### Azure OpenAI (EU-gehostet)

Dieselben OpenAI-Modelle, gehostet in der EU (Region Schweden-Zentral).

**Warum Azure OpenAI wählen:**
- **EU-Hosting:** Daten werden innerhalb der EU verarbeitet
- **Enterprise-Funktionen:** Azure-Sicherheit, Compliance, SLAs
- **Dieselben Modelle:** GPT-4.1, GPT-4.1 Mini, GPT-5 Mini/Nano

### Anthropic

Claude-Modelle zeichnen sich durch Sicherheit, Befolgung von Anweisungen und komplexe Schlussfolgerungen aus.

**Claude Haiku 4.5** ⭐ **Empfohlen**
- **Reale Leistung:** Antworten unter einer Sekunde, hervorragendes Geschwindigkeit-zu-Intelligenz-Verhältnis
- **Am besten für:** Geschwindigkeitskritische Bereitstellungen, Hochvolumen-Anwendungsfälle
- **Warum es funktioniert:** Schnell, erschwinglich, starke Constitutional AI-Sicherheit

**Claude Sonnet 4.5**
- **Reale Leistung:** Hervorragend für komplexe Agenten-Workflows und Tool-Nutzung
- **Am besten für:** Mehrstufige Schlussfolgerung, komplexe Verfahren, Codierungsaufgaben
- **Überlegungen:** Kann Latenzspitzen unter starker Last haben; überwachen du Timeouts in der Produktion
- **Erweitertes Denken:** Unterstützt längere Schlussfolgerungsketten für komplexe Probleme

<Note>
Claude-Modelle sind **konversationeller und reichhaltiger** in ihren Antworten im Vergleich zu OpenAI-Modellen. du liefern natürlich vollständigere, nuanciertere Antworten. Dies macht sie hervorragend für ansprechende Kundeninteraktionen, aber sie entschuldigen sich möglicherweise gelegentlich zu viel. Teste mit deinem spezifischen Anwendungsfall, ob der Gesprächsstil deinen Anforderungen entspricht.
</Note>

### Groq (Ultra-niedrige Latenz)

Open-Source-Modelle auf benutzerdefinierter Hardware für maximale Geschwindigkeit.

**Llama 3.1 8B Instant** ⭐ **Am schnellsten**
- **Reale Leistung:** Antwortzeiten unter 500ms, Hunderte von Token/Sekunde
- **Am besten für:** Einfache Qualifizierung, IVR, Routing, Hochvolumen-Szenarien
- **Kompromiss:** Weniger intelligent als GPT-4.1 oder Claude
- **Achten du auf:** Gelegentliche Latenzspitzen unter starker Last

**Llama 3.3 70B Versatile**
- **Reale Leistung:** Bessere Schlussfolgerung als 8B bei Beibehaltung der Groq-Geschwindigkeit
- **Am besten für:** Wenn du mehr Intelligenz als 8B benötigen, aber Groqs Geschwindigkeitsvorteil wünschen

**GPT-OSS Serie** (20B, 120B)
- **Reale Leistung:** 20B-Modell ist sehr schnell auf Groq-Hardware, ähnlich wie Llama-Geschwindigkeiten
- **Status:** Open-Weight-OpenAI-Modelle mit Tool-Nutzungsunterstützung
- **Am besten für:** Schnelle Open-Source-Alternative mit Funktionsaufruf

<Tip>
**Groq ist perfekt für:** Entfernung von LLM-Engpässen, wenn unter 800ms kritisch ist und Aufgaben unkompliziert sind (Qualifizierung, Routing, Datenerfassung).
</Tip>

---

## Modellparameter

Klicke auf **Modellparameter**, um auf erweiterte Konfigurationsoptionen zuzugreifen, die das Verhalten des Modells steuern.

### Temperature

Steuert die Zufälligkeit in Antworten (Bereich: 0,0 bis 2,0)

- **0,0 (Empfohlen):** Deterministische, konsistente Antworten
  - Verwendung für: Die meisten Sprachagenten, Tool-Aufrufe, Aktionsausführung
  - Maximiert Zuverlässigkeit für Weiterleitungen, Buchungen und API-Aufrufe
  - Gewährleistet konsistentes Verhalten und vorhersehbare Antworten

- **0,1 - 0,3:** Leicht variiert, aber immer noch sehr konsistent
  - Verwendung für: Agenten, die leichte natürliche Variation benötigen
  - Immer noch zuverlässig für Tool-Aufrufe

- **0,4 - 0,7:** Kreativer und variabler
  - Verwendung für: Persönlichkeitsgetriebene Agenten, bei denen Kreativität wichtiger als Konsistenz ist
  - Zuverlässigkeit von Tool-Aufrufen nimmt ab

- **0,8+:** Hochkreativ, unvorhersehbar
  - Vermeiden du für produktive Sprachagenten
  - Tool-Aufrufe werden unzuverlässig

<Note>
**Standard-Empfehlung:** Verwende 0,0, es sei denn, dein Agent benötigt mehr menschenähnliche Kreativität. Temperature über 0 reduziert die Zuverlässigkeit von Tool-Aufrufen (Weiterleitungen, Buchungen, Aktionen).
</Note>

---

## Auswahl des richtigen Modells

### Entscheidungsrahmen

Verwende diesen Rahmen zur Auswahl deines Modells:

<AccordionGroup>
  <Accordion title="1. Beginne mit dem richtigen Standard" icon="star" defaultOpen>
    **Für die meisten Anwendungsfälle beginnen du hier:**
    - **GPT-4.1 Mini** → Bestes Gleichgewicht zwischen Geschwindigkeit, Zuverlässigkeit und Kosten
    - **Claude Haiku 4.5** → Wenn du schnellere Antworten oder niedrigere Kosten benötigen

    **Nur upgraden, wenn du mehr Intelligenz benötigen:**
    - **GPT-4.1** → Komplexe mehrstufige Schlussfolgerung erforderlich
    - **Claude Sonnet 4.5** → Maximale Gesprächsqualität

    **Gehe nur schneller/günstiger, wenn nötig:**
    - **Groq Llama 3.1 8B** → Geschwindigkeit unter 500ms ist kritisch
  </Accordion>

  <Accordion title="2. An deinen Anwendungsfall anpassen" icon="brain">
    **Einfaches Routing / FAQ:**
    - Groq Llama 3.1 8B (am schnellsten)
    - Llama 3.3 70B (intelligenter)

    **Standard-Kundensupport (Häufigste):**
    - **GPT-4.1 Mini** ⭐ (empfohlen - bestes Gleichgewicht)
    - Claude Haiku 4.5 (schneller, konversationeller)

    **Komplexe Schlussfolgerung / Technischer Support:**
    - GPT-4.1 (wenn Mini nicht ausreicht)
    - Claude Sonnet 4.5 (maximale Qualität)

    **Persönlichkeitskritisch / Markensensitiv:**
    - Claude Sonnet 4.5 (reichste, konversationellste)
    - GPT-4.1 (wenn du Schlussfolgerung + Persönlichkeit benötigen)
  </Accordion>

  <Accordion title="3. EU-Hosting" icon="shield">
    **GDPR-konformes EU-Hosting erforderlich?**
    - **Azure OpenAI** ist der einzige Anbieter mit EU-Hosting
    - Alle GPT-4.1, GPT-4.1 Mini und GPT-5 Modelle verfügbar
  </Accordion>
</AccordionGroup>

### Häufige Modellkombinationen

Viele Kunden verwenden verschiedene Modelle für verschiedene Agenten:

```text wrap
Standard-Support → GPT-4.1 Mini (bester Standard für die meisten Agenten)
Hochvolumen-Routing → Groq Llama 3.1 8B (geschwindigkeitskritisch, einfache Aufgaben)
Terminbuchung → GPT-4.1 Mini oder Claude Haiku 4.5 (zuverlässiger Tool-Aufruf)
Komplexe Fehlerbehebung → GPT-4.1 (wenn Sie mehr Schlussfolgerung benötigen)
Marken-/Persönlichkeitskritisch → Claude Sonnet 4.5 (reichste Gespräche)
```

---

## Testen der Modellleistung

### A/B-Tests von Modellen

Um Modelle wissenschaftlich zu vergleichen:

1. **Duplizieren du deinen Agenten** im Dashboard
2. **Ändern du nur das Modell** bei einer Version
3. **Halte alle anderen Einstellungen identisch** (Anweisungen, Stimme, Aktionen)
4. **Führen du identische Testszenarien** bei beiden durch
5. **Vergleichen du:**
   - Antwortqualität und Genauigkeit
   - Latenz und Geschwindigkeit
   - Natürlichkeit der Konversation
   - Zuverlässigkeit der Aktionsauslösung

### Bewertungskriterien

Bewerten du jedes Modell nach:

| Kriterium | Worauf zu achten ist |
|----------|-----------------|
| **Genauigkeit** | Versteht es Anfragen korrekt? |
| **Befolgung von Anweisungen** | Folgt es deinen System-Prompt-Regeln? |
| **Latenz** | Wie schnell antwortet es? |
| **Kontextbeibehaltung** | Erinnert es sich an frühere Gespräche? |
| **Aktions-Timing** | Löst es Aktionen zu den richtigen Momenten aus? |
| **Fehlerbehandlung** | Wie geht es mit unklaren Anfragen um? |

---

## Best Practices

<AccordionGroup>
  <Accordion title="Beginne mit GPT-4.1 Mini" icon="rocket">
    Für die meisten Sprachagenten beginnen du mit:
    - **Modell:** GPT-4.1 Mini
    - **Temperature:** 0,0 (oder 0,7 für mehr Persönlichkeit)

    Wechseln du nur, wenn Tests zeigen, dass du mehr Intelligenz oder schnellere Geschwindigkeit benötigen.
  </Accordion>

  <Accordion title="Gib nicht zu viel für Intelligenz aus" icon="scale-balanced">
    **Klein anfangen, nur bei Bedarf upgraden:**
    - Die meisten Anwendungsfälle funktionieren hervorragend mit GPT-4.1 Mini
    - Nur auf GPT-4.1 oder Claude Sonnet 4.5 upgraden, wenn Mini deine Komplexität nicht bewältigen kann
    - Verwende Groq für einfaches Routing/FAQ, wo Geschwindigkeit wichtiger als Intelligenz ist

    Passe Fähigkeit an Anforderung an – zahlen du nicht für Intelligenz, die du nicht benötigen.
  </Accordion>

  <Accordion title="Überwache die reale Leistung" icon="chart-line">
    Verwende Analytics zur Verfolgung:
    - Durchschnittliche Antwortzeit
    - Aktionserfolgsraten
    - Weiterleitungsraten (hohe Weiterleitungen können auf Schlussfolgerungsprobleme hinweisen)
    - Kundenzufriedenheitswerte

    Wechseln du Modelle, wenn sich Metriken verschlechtern.
  </Accordion>

  <Accordion title="Berücksichtigen du regionale Bereitstellung" icon="globe">
    Wenn du globale Kunden bedienen:
    - Verwende EU-gehostete Modelle für europäische Anrufer (GDPR)
    - Erwägen du regionale Azure-Bereitstellungen für Enterprise-Compliance
    - Berücksichtigen du Latenz von der Modell-Hosting-Region zu Kunden
  </Accordion>

  <Accordion title="Dokumentieren du Modelländerungen" icon="file-pen">
    Wenn du Modelle in der Produktion ändern:
    - Notieren du das Datum und den Grund in der Agentenbeschreibung
    - Überwache Metriken 24-48 Stunden danach
    - Halte die vorherige Modell-ID für Rollback dokumentiert
    - Teste gründlich, bevor du Hochvolumen-Agenten umstellen
  </Accordion>
</AccordionGroup>

---

## Fehlerbehebung bei Modellproblemen

### Agentenantworten sind zu ausführlich

**Lösungen:**
- Füge den Anweisungen hinzu: "Halte jede Antwort unter 25 Sekunden"
- Verwende Temperature 0,0 für fokussiertere, prägnantere Antworten
- Erwägen du ein schnelleres Modell, das Kürze fördert

### Agent missversteht Anfragen

**Lösungen:**
- Wechseln du zu einem leistungsfähigeren Modell (GPT-4.1, Claude Sonnet 4.5)
- Verbessern du Anweisungen mit spezifischeren Beispielen
- Füge Keyword-Boosting in Transcriber-Einstellungen hinzu
- Überprüfe zuerst die Transkriptionsgenauigkeit (könnte STT-Problem sein, nicht LLM)

### Agent folgt Anweisungen nicht

**Lösungen:**
- Claude-Modelle normalerweise besser bei Befolgung von Anweisungen
- Vereinfachen und klären du Anweisungen
- Verwende Aufzählungslisten statt Absätze
- Füge explizite Beispiele für korrektes Verhalten hinzu
- Verwende Temperature 0,0 für maximale Konsistenz

### Hohe Latenz / Langsame Antworten

**Lösungen:**
- Wechseln du zu einem schnelleren Modell (Groq Llama 3.1 8B, Claude Haiku 4.5)
- Prüfen du, ob das Problem Modell- oder Netzwerklatenz ist (teste mit verschiedenen Anbietern)

### Agent wiederholt dieselben Phrasen

**Lösungen:**
- Füge Anweisung hinzu: "Variiere deine Formulierung; vermeide sich wiederholende Ausdrücke"
- Erwägen du ein anderes Modell (einige haben bessere Diversität)
- Überprüfe, ob Anweisungen unbeabsichtigt Wiederholungen verursachen

---

## Modell-Updates und Versionierung

### Anbieter-Modell-Updates

Modellanbieter aktualisieren regelmäßig ihre Angebote:
- **Kleinere Updates** verbessern oft die Leistung ohne Breaking Changes
- **Hauptversionsänderungen** (z.B. GPT-4 → GPT-5) können Tests erfordern
- itellicoAI benachrichtigt Kunden vor automatischen Versionsupdates

### Kontrolle von Modellversionen

Einige Anbieter ermöglichen es dir, auf bestimmte Versionen festzulegen:
- **Latest:** Verwende immer die neueste Version (Standard, empfohlen)
- **Pinned:** Bleiben du bei einer bestimmten Version (verwenden du dies, wenn du stark für dieses Modell optimiert haben)

### Deprecation-Richtlinie

Wenn Anbieter Modelle außer Betrieb nehmen:
1. itellicoAI benachrichtigt betroffene Kunden im Voraus
2. Empfohlener Migrationspfad wird bereitgestellt
3. Agenten werden automatisch zum Nachfolgemodell verschoben, wenn keine Aktion unternommen wird
4. Migrationshilfe vom Support verfügbar

---

## Nächste Schritte

<CardGroup cols={2}>
  <Card title="Stimme auswählen" icon="microphone" href="/de/build/voice-speech/select-voice">
    Konfigurieren du, wie dein Agent mit der Stimmenauswahl klingt
  </Card>
  <Card title="Transcriber-Konfiguration" icon="ear" href="/de/build/voice-speech/transcriber">
    Wähle Transkriptionsmodelle zum Zuhören von Kunden
  </Card>
  <Card title="Stimmeneinstellungen" icon="sliders" href="/de/build/voice-speech/voice-settings">
    Feinabstimmung von Geschwindigkeit, Tonhöhe und Klangfarbe für deine Stimme
  </Card>
  <Card title="Teste deinen Agenten" icon="vial" href="/de/test/web-simulator">
    Teste die Modellleistung mit Web-Anrufen
  </Card>
</CardGroup>
