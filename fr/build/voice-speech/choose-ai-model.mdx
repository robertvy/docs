---
title: "Choisir le modèle IA"
description: "Sélectionne et Configure le modèle de langage qui alimente le raisonnement de ton agent"
icon: "brain"
"keywords": "sélection de modèle IA, modèles de langage, GPT-4, Claude, sélection LLM, intelligence IA, configuration de modèle, cerveau d'agent"
"og:description": "Sélectionne et Configure le modèle de langage qui alimente le raisonnement de ton agent IA, de GPT-4 à Claude et plus encore."
---

## Présentation

Le modèle IA (LLM) est le cerveau de ton agent vocal. Il traite ce que disent les clients, comprend leur intention, raisonne sur la meilleure réponse et décide quand entreprendre des actions. Choisir le bon modèle signifie équilibrer les exigences de performance, latence, coût et conformité.

<Note>
La sélection du modèle se fait sous **Modèles > Modèle** dans ton configuration d'agent. Les modifications s'appliquent immédiatement - aucune étape de publication séparée n'est requise.
</Note>

## Comprendre les modèles de langage

Les modèles de langage sont entraînés sur de vastes quantités de texte pour comprendre et générer le langage humain. Dans les agents vocaux, le LLM interprète les demandes des clients, raisonne sur la meilleure réponse basée sur tes instructions et ton base de connaissances, décide quand utiliser des actions comme les transferts ou réservations, génère des réponses conversationnelles naturelles et maintient le contexte tout au long de la conversation.

Différents modèles excellent dans différentes tâches. Certains privilégient la vitesse, d'autres la précision, et certains offrent le meilleur équilibre pour l'IA conversationnelle.

---

## Modèles recommandés

Basé sur les performances réelles de milliers d'agents vocaux, voici les modèles éprouvés pour différents cas d'usage :

<AccordionGroup>
  <Accordion title="Meilleur pour la plupart des cas d'usage : GPT-4.1 Mini" icon="star" defaultOpen>
    **Notre meilleure recommandation** pour les agents vocaux en production.

    **Pourquoi ça fonctionne :**
    - Excellente latence (~700-800ms temps de réponse)
    - Taux de succès 70%+ dans l'appel de fonction (transferts, réservations, actions)
    - Bon suivi des instructions
    - Coût abordable

    **Utiliser pour :**
    - Support client
    - Réservation de rendez-vous
    - Traitement de commande
    - La plupart des scénarios conversationnels

    **Disponible sur :** OpenAI, Azure OpenAI (hébergé en UE)
  </Accordion>

  <Accordion title="Pour tâches complexes : GPT-4.1" icon="brain">
    Quand tu as besoin d'intelligence et de raisonnement maximum.

    **Pourquoi ça fonctionne :**
    - Meilleur raisonnement de sa catégorie et logique multi-étapes
    - Gère le dépannage complexe
    - Compréhension de contexte supérieure

    **Compromis :**
    - Latence plus élevée que GPT-4.1 Mini
    - Coût plus élevé par conversation

    **Utiliser pour :**
    - Support technique avec diagnostics complexes
    - Conversations de vente multi-étapes
    - Tâches nécessitant un raisonnement approfondi

    **Disponible sur :** OpenAI, Azure OpenAI (hébergé en UE)
  </Accordion>

  <Accordion title="Rapide et abordable : Claude Haiku 4.5" icon="bolt">
    Le modèle le plus rapide d'Anthropic avec de bonnes performances.

    **Pourquoi ça fonctionne :**
    - Temps de réponse sous la seconde
    - Bon équilibre entre vitesse et intelligence
    - IA constitutionnelle pour des réponses plus sûres
    - Coût plus bas que Sonnet

    **Utiliser pour :**
    - Centres d'appels à haut volume
    - Applications critiques en vitesse
    - Déploiements soucieux du budget

    **Disponible sur :** Anthropic
  </Accordion>

  <Accordion title="Ultra-rapide Open Source : Groq Llama 3.1 8B" icon="rocket">
    Option la plus rapide disponible, alimentée par le matériel personnalisé de Groq.

    **Pourquoi ça fonctionne :**
    - Temps de réponse sous 500ms
    - Gère des centaines de tokens par seconde
    - Modèle open source
    - Coût très bas

    **Compromis :**
    - Moins intelligent que GPT-4.1 ou Claude
    - Pics de latence occasionnels sous charge
    - Meilleur pour les conversations simples

    **Utiliser pour :**
    - Appels de qualification simples
    - IVR et routage
    - Scénarios à haut volume et faible complexité

    **Disponible sur :** Groq
  </Accordion>

  <Accordion title="Plus d'intelligence de Groq : Llama 3.3 70B" icon="layer-group">
    Meilleur raisonnement tout en conservant l'avantage de vitesse de Groq.

    **Pourquoi ça fonctionne :**
    - Montée en qualité par rapport au modèle 8B
    - Toujours rapide sur l'infrastructure Groq
    - Bon compromis

    **Utiliser pour :**
    - Quand la qualité de Llama 3.1 8B n'est pas suffisante
    - Besoin de vitesse mais plus d'intelligence

    **Disponible sur :** Groq
  </Accordion>
</AccordionGroup>

<Tip>
**Guide de décision rapide :**
- Commence avec **GPT-4.1 Mini** → fiable, rapide, excellent pour la plupart des cas d'usage
- Besoin de plus de raisonnement ? → **GPT-4.1**
- Besoin de plus rapide/moins cher ? → **Claude Haiku 4.5**
- Besoin du plus rapide ? → **Groq Llama 3.1 8B** (mais moins intelligent)
</Tip>

---

## Interface de sélection de modèle

### Catalogue de fournisseurs

L'interface de sélection de modèle regroupe les fournisseurs avec des métadonnées utiles :

<CardGroup cols={2}>
  <Card title="Icônes de fournisseur" icon="building">
    Branding visuel pour OpenAI, Anthropic, Groq, Azure et plus
  </Card>
  <Card title="Badge hébergé en UE" icon="location-dot">
    Indique les modèles qui traitent les données dans les régions UE
  </Card>
  <Card title="Nombre de modèles" icon="list-ol">
    Montre combien de modèles sont disponibles de chaque fournisseur
  </Card>
  <Card title="Sélection active" icon="check">
    Met en évidence ton modèle actuellement sélectionné
  </Card>
</CardGroup>

### Filtrage et recherche

Clique sur un fournisseur pour filtrer la table de modèles à ce fournisseur uniquement. Utilise la boîte de recherche pour trouver rapidement des modèles spécifiques par nom ou capacité.

---

## Détails des fournisseurs de modèles

### OpenAI

Les modèles OpenAI offrent le meilleur équilibre entre fiabilité et appel de fonction pour les agents vocaux.

**GPT-4.1 Mini** ⭐ **Recommandé**
- **Performance réelle :** ~700-800ms temps de réponse, taux de succès d'appel de fonction 70%+
- **Meilleur pour :** Agents vocaux de production - support, réservation, ventes
- **Pourquoi ça fonctionne :** Fiabilité prouvée, excellente utilisation des outils, bonne latence

**GPT-4.1**
- **Performance réelle :** Latence plus élevée que Mini mais raisonnement supérieur
- **Meilleur pour :** Conversations complexes multi-étapes, support technique
- **Compromis :** Coût et latence plus élevés pour plus d'intelligence

**Série GPT-5** (Mini, Nano)
- **Statut :** Modèles de nouvelle génération avec raisonnement avancé
- **Considérations :** GPT-5 a une latence plus élevée (~1s+) ; GPT-5 Mini offre un meilleur équilibre
- **Meilleur pour :** Tâches où l'intelligence compte plus que la vitesse

**Modèles hérités** (GPT-4o, GPT-4o Mini)
- **Statut :** Toujours fonctionnels mais considérez les séries GPT-4.1/5 pour les nouveaux agents

### Azure OpenAI (hébergé en UE)

Mêmes modèles OpenAI hébergés en UE (région Suède centrale).

**Pourquoi choisir Azure OpenAI :**
- **Hébergement UE :** Données traitées dans l'UE
- **Fonctionnalités d'entreprise :** Sécurité Azure, conformité, SLA
- **Mêmes modèles :** GPT-4.1, GPT-4.1 Mini, GPT-5 Mini/Nano

### Anthropic

Les modèles Claude excellent en matière de sécurité, suivi des instructions et raisonnement complexe.

**Claude Haiku 4.5** ⭐ **Recommandé**
- **Performance réelle :** Réponses sous la seconde, excellent ratio vitesse-intelligence
- **Meilleur pour :** Déploiements critiques en vitesse, cas d'usage à haut volume
- **Pourquoi ça fonctionne :** Rapide, abordable, forte sécurité IA constitutionnelle

**Claude Sonnet 4.5**
- **Performance réelle :** Excellent pour workflows d'agent complexes et utilisation d'outils
- **Meilleur pour :** Raisonnement multi-étapes, procédures complexes, tâches de codage
- **Considérations :** Peut avoir des pics de latence sous charge lourde ; surveiller les timeouts en production
- **Pensée étendue :** Supporte des chaînes de raisonnement plus longues pour problèmes complexes

<Note>
Les modèles Claude sont **plus conversationnels et riches** dans leurs réponses comparés aux modèles OpenAI. Ils fournissent naturellement des réponses plus complètes et nuancées. Cela les rend excellents pour des interactions client engageantes, mais ils peuvent parfois s'excuser excessivement. Teste avec ton cas d'usage spécifique pour voir si le style conversationnel correspond à tes besoins.
</Note>

### Groq (latence ultra-faible)

Modèles open source sur matériel personnalisé pour vitesse maximale.

**Llama 3.1 8B Instant** ⭐ **Le plus rapide**
- **Performance réelle :** Temps de réponse sous 500ms, centaines de tokens/seconde
- **Meilleur pour :** Qualification simple, IVR, routage, scénarios à haut volume
- **Compromis :** Moins intelligent que GPT-4.1 ou Claude
- **Surveiller :** Pics de latence occasionnels sous charge lourde

**Llama 3.3 70B Versatile**
- **Performance réelle :** Meilleur raisonnement que 8B tout en gardant la vitesse de Groq
- **Meilleur pour :** Quand tu as besoin de plus d'intelligence que 8B mais voulez l'avantage de vitesse de Groq

**Série GPT-OSS** (20B, 120B)
- **Performance réelle :** Le modèle 20B est super rapide sur le matériel Groq, similaire aux vitesses Llama
- **Statut :** Modèles OpenAI à poids ouverts avec support d'utilisation d'outils
- **Meilleur pour :** Alternative open source rapide avec appel de fonction

<Tip>
**Groq est parfait pour :** supprimer les goulots d'étranglement LLM quand moins de 800ms est critique et les tâches sont simples (qualification, routage, collecte de données).
</Tip>

---

## Paramètres du modèle

Clique sur **Paramètres du modèle** pour accéder aux options de configuration avancées qui contrôlent le comportement du modèle.

### Température

Contrôle l'aléatoire dans les réponses (plage : 0.0 à 2.0)

- **0.0 (Recommandé) :** Réponses déterministes et cohérentes
  - Utiliser pour : La plupart des agents vocaux, appel d'outils, exécution d'actions
  - Maximise la fiabilité pour les transferts, réservations et appels API
  - Assure un comportement cohérent et des réponses prévisibles

- **0.1 - 0.3 :** Légèrement varié mais toujours très cohérent
  - Utiliser pour : Agents nécessitant une légère variation naturelle
  - Toujours fiable pour l'appel d'outils

- **0.4 - 0.7 :** Plus créatif et varié
  - Utiliser pour : Agents axés sur la personnalité où la créativité compte plus que la cohérence
  - La fiabilité de l'appel d'outils diminue

- **0.8+ :** Hautement créatif, imprévisible
  - Éviter pour les agents vocaux de production
  - L'appel d'outils devient peu fiable

<Note>
**Recommandation par défaut :** Utilise 0.0 sauf si ton agent a besoin de plus de créativité humaine. Une température supérieure à 0 réduit la fiabilité de l'appel d'outils (transferts, réservations, actions).
</Note>

---

## Choisir le bon modèle

### Cadre de décision

Utilise ce cadre pour sélectionner ton modèle :

<AccordionGroup>
  <Accordion title="1. Commencer avec le bon défaut" icon="star" defaultOpen>
    **Pour la plupart des cas d'usage, Commence ici :**
    - **GPT-4.1 Mini** → Meilleur équilibre vitesse, fiabilité et coût
    - **Claude Haiku 4.5** → Quand tu as besoin de réponses plus rapides ou coût plus bas

    **Montez en gamme seulement si tu as besoin de plus d'intelligence :**
    - **GPT-4.1** → Raisonnement complexe multi-étapes requis
    - **Claude Sonnet 4.5** → Qualité conversationnelle maximale

    **Va plus rapide/moins cher seulement si nécessaire :**
    - **Groq Llama 3.1 8B** → Vitesse sous 500ms critique
  </Accordion>

  <Accordion title="2. Correspondre à ton cas d'usage" icon="brain">
    **Routage simple / FAQ :**
    - Groq Llama 3.1 8B (plus rapide)
    - Llama 3.3 70B (plus intelligent)

    **Support client standard (le plus courant) :**
    - **GPT-4.1 Mini** ⭐ (recommandé - meilleur équilibre)
    - Claude Haiku 4.5 (plus rapide, plus conversationnel)

    **Raisonnement complexe / support technique :**
    - GPT-4.1 (quand Mini n'est pas suffisant)
    - Claude Sonnet 4.5 (qualité maximale)

    **Critique pour la personnalité / sensible à la marque :**
    - Claude Sonnet 4.5 (le plus riche, le plus conversationnel)
    - GPT-4.1 (quand tu as besoin de raisonnement + personnalité)
  </Accordion>

  <Accordion title="3. Hébergement UE" icon="shield">
    **Besoin d'hébergement UE conforme RGPD ?**
    - **Azure OpenAI** est le seul fournisseur avec hébergement UE
    - Tous les modèles GPT-4.1, GPT-4.1 Mini et GPT-5 disponibles
  </Accordion>
</AccordionGroup>

### Combinaisons de modèles courantes

De nombreux clients utilisent différents modèles pour différents agents :

```text wrap
Support standard → GPT-4.1 Mini (meilleur défaut pour la plupart des agents)
Routage haut volume → Groq Llama 3.1 8B (critique vitesse, tâches simples)
Réservation rendez-vous → GPT-4.1 Mini ou Claude Haiku 4.5 (appel d'outils fiable)
Dépannage complexe → GPT-4.1 (quand vous avez besoin de plus de raisonnement)
Critique marque/personnalité → Claude Sonnet 4.5 (conversations les plus riches)
```

---

## Tester les performances du modèle

### Tests A/B de modèles

Pour comparer les modèles scientifiquement :

1. **Dupliquez ton agent** dans le tableau de bord
2. **Change uniquement le modèle** sur une version
3. **Garde tous les autres paramètres identiques** (instructions, voix, actions)
4. **Exécute des scénarios de test identiques** sur les deux
5. **Compare :**
   - Qualité et précision des réponses
   - Latence et vitesse
   - Naturel de la conversation
   - Fiabilité du déclenchement d'actions

### Critères d'évaluation

Évaluez chaque modèle sur :

| Critère | Ce qu'il faut rechercher |
|----------|-----------------|
| **Précision** | Comprend-il correctement les demandes ? |
| **Adhésion aux instructions** | Suit-il les règles de ton prompt système ? |
| **Latence** | À quelle vitesse répond-il ? |
| **Rétention de contexte** | Se souvient-il de la conversation antérieure ? |
| **Timing des actions** | Déclenche-t-il les actions aux bons moments ? |
| **Gestion d'erreur** | Comment gère-t-il les demandes peu claires ? |

---

## Meilleures pratiques

<AccordionGroup>
  <Accordion title="Commencer avec GPT-4.1 Mini" icon="rocket">
    Pour la plupart des agents vocaux, Commence avec :
    - **Modèle :** GPT-4.1 Mini
    - **Température :** 0.0 (ou 0.7 pour plus de personnalité)

    Change seulement si les tests montrent que tu as besoin de plus d'intelligence ou de vitesse plus rapide.
  </Accordion>

  <Accordion title="Ne pas trop dépenser en intelligence" icon="scale-balanced">
    **Commence petit, montez en gamme seulement si nécessaire :**
    - La plupart des cas d'usage fonctionnent très bien avec GPT-4.1 Mini
    - Montez vers GPT-4.1 ou Claude Sonnet 4.5 seulement si Mini ne peut pas gérer ton complexité
    - Utilise Groq pour le routage simple/FAQ où la vitesse compte plus que l'intelligence

    Correspondez la capacité à l'exigence - ne payez pas pour l'intelligence dont vous n'avez pas besoin.
  </Accordion>

  <Accordion title="Surveiller les performances réelles" icon="chart-line">
    Utilise l'analytique pour suivre :
    - Temps de réponse moyen
    - Taux de succès des actions
    - Taux de transfert (transferts élevés peuvent indiquer des problèmes de raisonnement)
    - Scores de satisfaction client

    Change de modèle si les métriques se dégradent.
  </Accordion>

  <Accordion title="Considérer le déploiement régional" icon="globe">
    Si tu sers des clients mondiaux :
    - Utilise des modèles hébergés en UE pour les appelants européens (RGPD)
    - Considérez les déploiements Azure régionaux pour la conformité d'entreprise
    - Tenez compte de la latence depuis la région d'hébergement du modèle vers les clients
  </Accordion>

  <Accordion title="Documenter les changements de modèle" icon="file-pen">
    Lors du changement de modèles en production :
    - Note la date et la raison dans la description de l'agent
    - Surveillez les métriques pendant 24-48 heures après
    - Garde l'ID du modèle précédent documenté pour rollback
    - Teste minutieusement avant de changer les agents à haut volume
  </Accordion>
</AccordionGroup>

---

## Dépannage des problèmes de modèle

### Les réponses de l'agent sont trop verbeuses

**Solutions :**
- Ajoute aux instructions : "Garde chaque réponse sous 25 secondes"
- Utilise température 0.0 pour des réponses plus focalisées et concises
- Considérez un modèle plus rapide qui encourage la brièveté

### L'agent comprend mal les demandes

**Solutions :**
- Passe à un modèle de capacité supérieure (GPT-4.1, Claude Sonnet 4.5)
- Améliore les instructions avec des exemples plus spécifiques
- Ajoute un boost de mots-clés dans les paramètres du transcripteur
- Examine d'abord la précision de transcription (peut être un problème STT, pas LLM)

### L'agent ne suit pas les instructions

**Solutions :**
- Les modèles Claude généralement meilleurs pour l'adhésion aux instructions
- Simplifiez et Clarifie les instructions
- Utilise des listes à puces au lieu de paragraphes
- Ajoute des exemples explicites de comportement correct
- Utilise température 0.0 pour cohérence maximale

### Latence élevée / réponses lentes

**Solutions :**
- Passe à un modèle plus rapide (Groq Llama 3.1 8B, Claude Haiku 4.5)
- Vérifie si le problème est la latence du modèle ou du réseau (Teste avec différents fournisseurs)

### L'agent répète les mêmes phrases

**Solutions :**
- Ajoute l'instruction : "Variez ton formulation ; Évite les expressions répétitives"
- Considérez un modèle différent (certains ont meilleure diversité)
- Examine si les instructions causent involontairement la répétition

---

## Mises à jour et versionnage des modèles

### Mises à jour de modèles des fournisseurs

Les fournisseurs de modèles mettent régulièrement à jour leurs offres :
- Les **mises à jour mineures** améliorent souvent les performances sans changements cassants
- Les **changements de version majeure** (ex : GPT-4 → GPT-5) peuvent nécessiter des tests
- itellicoAI notifie les clients avant les mises à jour de version automatiques

### Contrôle des versions de modèle

Certains fournisseurs te permettent d'épingler à des versions spécifiques :
- **Dernière :** Toujours utiliser la version la plus récente (par défaut, recommandé)
- **Épinglée :** Rester sur une version spécifique (Utilise si tu as lourdement optimisé pour ce modèle)

### Politique de dépréciation

Quand les fournisseurs déprécient des modèles :
1. itellicoAI notifie les clients affectés à l'avance
2. Chemin de migration recommandé fourni
3. Les agents sont automatiquement déplacés vers le modèle successeur si aucune action n'est prise
4. Assistance à la migration disponible via le support

---

## Prochaines étapes

<CardGroup cols={2}>
  <Card title="Sélectionner la voix" icon="microphone" href="/fr/build/voice-speech/select-voice">
    Configure comment sonne ton agent avec la sélection de voix
  </Card>
  <Card title="Configuration du transcripteur" icon="ear" href="/fr/build/voice-speech/transcriber">
    Choisis les modèles de transcription pour écouter les clients
  </Card>
  <Card title="Paramètres vocaux" icon="sliders" href="/fr/build/voice-speech/voice-settings">
    Affinez la vitesse, le ton et le timbre de ton voix
  </Card>
  <Card title="Teste ton agent" icon="vial" href="/fr/test/web-simulator">
    Teste les performances du modèle avec des appels web
  </Card>
</CardGroup>
