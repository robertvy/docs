---
title: "Scegli il Modello AI"
description: "Seleziona e configura il modello linguistico che alimenta il ragionamento del tuo agente"
icon: "brain"
"keywords": "selezione modello AI, modelli linguistici, GPT-4, Claude, selezione LLM, intelligenza AI, configurazione modello, cervello agente"
"og:description": "Seleziona e configura il modello linguistico che alimenta il ragionamento del tuo agente AI, da GPT-4 a Claude e altri."
---

## Panoramica

Il modello AI (LLM) è il cervello del tuo agente vocale. Elabora ciò che dicono i clienti, comprende le loro intenzioni, ragiona sulla migliore risposta e decide quando intraprendere azioni. Scegliere il modello giusto significa bilanciare prestazioni, latenza, costo e requisiti di conformità.

<Note>
La selezione del modello avviene sotto **Models > Model** nella configurazione del tuo agente. Le modifiche si applicano immediatamente—non è richiesto un passaggio di pubblicazione separato.
</Note>

## Comprendere i Modelli Linguistici

I modelli linguistici sono addestrati su enormi quantità di testo per comprendere e generare linguaggio umano. Negli agenti vocali, l'LLM interpreta le richieste dei clienti, ragiona sulla migliore risposta basandosi sulle tue istruzioni e sulla base di conoscenza, decide quando utilizzare azioni come trasferimenti o prenotazioni, genera risposte conversazionali naturali e mantiene il contesto durante tutta la conversazione.

Modelli diversi eccellono in compiti diversi. Alcuni danno priorità alla velocità, altri all'accuratezza, e alcuni offrono il miglior equilibrio per l'AI conversazionale.

---

## Modelli Consigliati

Basandosi sulle prestazioni reali di migliaia di agenti vocali, ecco i modelli comprovati per diversi casi d'uso:

<AccordionGroup>
  <Accordion title="Migliore per la Maggior Parte dei Casi d'Uso: GPT-4.1 Mini" icon="star" defaultOpen>
    **La nostra raccomandazione principale** per agenti vocali in produzione.

    **Perché funziona:**
    - Eccellente latenza (~700-800ms tempo di risposta)
    - 70%+ tasso di successo nella chiamata di funzioni (trasferimenti, prenotazioni, azioni)
    - Forte aderenza alle istruzioni
    - Costo accessibile

    **Usa per:**
    - Supporto clienti
    - Prenotazione appuntamenti
    - Elaborazione ordini
    - La maggior parte degli scenari conversazionali

    **Disponibile su:** OpenAI, Azure OpenAI (ospitato in UE)
  </Accordion>

  <Accordion title="Per Compiti Complessi: GPT-4.1" icon="brain">
    Quando hai bisogno di massima intelligenza e ragionamento.

    **Perché funziona:**
    - Ragionamento e logica multi-step migliori della categoria
    - Gestisce risoluzione problemi complessi
    - Comprensione del contesto superiore

    **Compromessi:**
    - Latenza più alta di GPT-4.1 Mini
    - Costo più alto per conversazione

    **Usa per:**
    - Supporto tecnico con diagnostica complessa
    - Conversazioni di vendita multi-step
    - Compiti che richiedono ragionamento profondo

    **Disponibile su:** OpenAI, Azure OpenAI (ospitato in UE)
  </Accordion>

  <Accordion title="Veloce ed Economico: Claude Haiku 4.5" icon="bolt">
    Il modello più veloce di Anthropic con prestazioni solide.

    **Perché funziona:**
    - Tempi di risposta inferiori al secondo
    - Buon equilibrio tra velocità e intelligenza
    - AI Costituzionale per risposte più sicure
    - Costo inferiore a Sonnet

    **Usa per:**
    - Call center ad alto volume
    - Applicazioni critiche per la velocità
    - Implementazioni attente al budget

    **Disponibile su:** Anthropic
  </Accordion>

  <Accordion title="Open Source Ultra-Veloce: Groq Llama 3.1 8B" icon="rocket">
    Opzione più veloce disponibile, alimentata dall'hardware personalizzato di Groq.

    **Perché funziona:**
    - Tempi di risposta inferiori a 500ms
    - Gestisce centinaia di token al secondo
    - Modello open source
    - Costo molto basso

    **Compromessi:**
    - Meno intelligente di GPT-4.1 o Claude
    - Occasionali picchi di latenza sotto carico
    - Migliore per conversazioni più semplici

    **Usa per:**
    - Chiamate di qualificazione semplici
    - IVR e routing
    - Scenari ad alto volume e bassa complessità

    **Disponibile su:** Groq
  </Accordion>

  <Accordion title="Più Intelligenza da Groq: Llama 3.3 70B" icon="layer-group">
    Miglior ragionamento mantenendo il vantaggio di velocità di Groq.

    **Perché funziona:**
    - Miglioramento di qualità rispetto al modello 8B
    - Ancora veloce sull'infrastruttura Groq
    - Buon compromesso

    **Usa per:**
    - Quando la qualità di Llama 3.1 8B non è sufficiente
    - Necessità di velocità ma maggiore intelligenza

    **Disponibile su:** Groq
  </Accordion>
</AccordionGroup>

<Tip>
**Guida decisionale rapida:**
- Inizia con **GPT-4.1 Mini** → affidabile, veloce, ottimo per la maggior parte dei casi d'uso
- Serve più ragionamento? → **GPT-4.1**
- Serve più veloce/economico? → **Claude Haiku 4.5**
- Serve il più veloce? → **Groq Llama 3.1 8B** (ma meno intelligente)
</Tip>

---

## Interfaccia di Selezione del Modello

### Catalogo dei Provider

L'interfaccia di selezione del modello raggruppa i provider con metadati utili:

<CardGroup cols={2}>
  <Card title="Icone Provider" icon="building">
    Branding visivo per OpenAI, Anthropic, Groq, Azure e altri
  </Card>
  <Card title="Badge Ospitato in UE" icon="location-dot">
    Indica i modelli che elaborano i dati all'interno delle regioni UE
  </Card>
  <Card title="Conteggio Modelli" icon="list-ol">
    Mostra quanti modelli sono disponibili da ciascun provider
  </Card>
  <Card title="Selezione Attiva" icon="check">
    Evidenzia il tuo modello attualmente selezionato
  </Card>
</CardGroup>

### Filtro e Ricerca

Clicca su un provider per filtrare la tabella dei modelli solo per quel fornitore. Usa la casella di ricerca per trovare rapidamente modelli specifici per nome o capacità.

---

## Dettagli dei Provider di Modelli

### OpenAI

I modelli OpenAI offrono il miglior equilibrio tra affidabilità e chiamata di funzioni per gli agenti vocali.

**GPT-4.1 Mini** ⭐ **Consigliato**
- **Prestazioni reali:** ~700-800ms tempo di risposta, 70%+ tasso di successo nella chiamata di funzioni
- **Migliore per:** Agenti vocali in produzione - supporto, prenotazioni, vendite
- **Perché funziona:** Affidabilità comprovata, eccellente uso degli strumenti, buona latenza

**GPT-4.1**
- **Prestazioni reali:** Latenza più alta di Mini ma ragionamento superiore
- **Migliore per:** Conversazioni multi-step complesse, supporto tecnico
- **Compromesso:** Costo e latenza più alti per maggiore intelligenza

**Serie GPT-5** (Mini, Nano)
- **Stato:** Modelli di prossima generazione con ragionamento avanzato
- **Considerazioni:** GPT-5 ha latenza più alta (~1s+); GPT-5 Mini offre un equilibrio migliore
- **Migliore per:** Compiti in cui l'intelligenza conta più della velocità

**Modelli legacy** (GPT-4o, GPT-4o Mini)
- **Stato:** Ancora funzionali ma considera la serie GPT-4.1/5 per nuovi agenti

### Azure OpenAI (Ospitato in UE)

Stessi modelli OpenAI ospitati in UE (regione Svezia Centrale).

**Perché scegliere Azure OpenAI:**
- **Hosting UE:** Dati elaborati all'interno dell'UE
- **Funzionalità enterprise:** Sicurezza Azure, conformità, SLA
- **Stessi modelli:** GPT-4.1, GPT-4.1 Mini, GPT-5 Mini/Nano

### Anthropic

I modelli Claude eccellono in sicurezza, aderenza alle istruzioni e ragionamento complesso.

**Claude Haiku 4.5** ⭐ **Consigliato**
- **Prestazioni reali:** Risposte inferiori al secondo, eccellente rapporto velocità-intelligenza
- **Migliore per:** Implementazioni critiche per la velocità, casi d'uso ad alto volume
- **Perché funziona:** Veloce, accessibile, forte sicurezza AI Costituzionale

**Claude Sonnet 4.5**
- **Prestazioni reali:** Eccellente per flussi di lavoro degli agenti complessi e uso degli strumenti
- **Migliore per:** Ragionamento multi-step, procedure complesse, compiti di codifica
- **Considerazioni:** Può avere picchi di latenza sotto carico pesante; monitora i timeout in produzione
- **Pensiero esteso:** Supporta catene di ragionamento più lunghe per problemi complessi

<Note>
I modelli Claude sono **più conversazionali e ricchi** nelle loro risposte rispetto ai modelli OpenAI. Forniscono naturalmente risposte più complete e sfumate. Questo li rende eccellenti per interazioni coinvolgenti con i clienti, ma possono occasionalmente scusarsi troppo. Testa con il tuo caso d'uso specifico per vedere se lo stile conversazionale si adatta alle tue esigenze.
</Note>

### Groq (Latenza Ultra-Bassa)

Modelli open-source su hardware personalizzato per massima velocità.

**Llama 3.1 8B Instant** ⭐ **Il Più Veloce**
- **Prestazioni reali:** Tempi di risposta inferiori a 500ms, centinaia di token/secondo
- **Migliore per:** Qualificazione semplice, IVR, routing, scenari ad alto volume
- **Compromesso:** Meno intelligente di GPT-4.1 o Claude
- **Attenzione a:** Occasionali picchi di latenza sotto carico pesante

**Llama 3.3 70B Versatile**
- **Prestazioni reali:** Miglior ragionamento di 8B mantenendo la velocità di Groq
- **Migliore per:** Quando serve più intelligenza di 8B ma si vuole il vantaggio di velocità di Groq

**Serie GPT-OSS** (20B, 120B)
- **Prestazioni reali:** Il modello 20B è super veloce sull'hardware Groq, velocità simili a Llama
- **Stato:** Modelli OpenAI a peso aperto con supporto per l'uso degli strumenti
- **Migliore per:** Alternativa open-source veloce con chiamata di funzioni

<Tip>
**Groq è perfetto per:** rimuovere i colli di bottiglia dell'LLM quando tempi inferiori a 800ms sono critici e i compiti sono semplici (qualificazione, routing, raccolta dati).
</Tip>

---

## Parametri del Modello

Clicca su **Model Parameters** per accedere alle opzioni di configurazione avanzate che controllano il comportamento del modello.

### Temperatura

Controlla la casualità nelle risposte (intervallo: 0.0 a 2.0)

- **0.0 (Consigliato):** Risposte deterministiche e coerenti
  - Usa per: La maggior parte degli agenti vocali, chiamata di strumenti, esecuzione di azioni
  - Massimizza l'affidabilità per trasferimenti, prenotazioni e chiamate API
  - Assicura comportamento coerente e risposte prevedibili

- **0.1 - 0.3:** Leggermente variato ma ancora altamente coerente
  - Usa per: Agenti che necessitano di leggera variazione naturale
  - Ancora affidabile per la chiamata di strumenti

- **0.4 - 0.7:** Più creativo e variato
  - Usa per: Agenti guidati dalla personalità dove la creatività conta più della coerenza
  - L'affidabilità della chiamata di strumenti diminuisce

- **0.8+:** Altamente creativo, imprevedibile
  - Evita per agenti vocali in produzione
  - La chiamata di strumenti diventa inaffidabile

<Note>
**Raccomandazione predefinita:** Usa 0.0 a meno che il tuo agente non necessiti di maggiore creatività simile a quella umana. Temperatura superiore a 0 riduce l'affidabilità della chiamata di strumenti (trasferimenti, prenotazioni, azioni).
</Note>

---

## Scegliere il Modello Giusto

### Framework Decisionale

Usa questo framework per selezionare il tuo modello:

<AccordionGroup>
  <Accordion title="1. Inizia con il Giusto Predefinito" icon="star" defaultOpen>
    **Per la maggior parte dei casi d'uso, inizia qui:**
    - **GPT-4.1 Mini** → Miglior equilibrio tra velocità, affidabilità e costo
    - **Claude Haiku 4.5** → Quando serve risposte più veloci o costo inferiore

    **Aggiorna solo se serve più intelligenza:**
    - **GPT-4.1** → Richiesto ragionamento multi-step complesso
    - **Claude Sonnet 4.5** → Qualità conversazionale massima

    **Vai più veloce/economico solo se necessario:**
    - **Groq Llama 3.1 8B** → Velocità inferiore a 500ms è critica
  </Accordion>

  <Accordion title="2. Abbina al Tuo Caso d'Uso" icon="brain">
    **Routing Semplice / FAQ:**
    - Groq Llama 3.1 8B (il più veloce)
    - Llama 3.3 70B (più intelligente)

    **Supporto Clienti Standard (Più Comune):**
    - **GPT-4.1 Mini** ⭐ (consigliato - miglior equilibrio)
    - Claude Haiku 4.5 (più veloce, più conversazionale)

    **Ragionamento Complesso / Supporto Tecnico:**
    - GPT-4.1 (quando Mini non è sufficiente)
    - Claude Sonnet 4.5 (qualità massima)

    **Critico per Personalità / Sensibile al Brand:**
    - Claude Sonnet 4.5 (più ricco, più conversazionale)
    - GPT-4.1 (quando serve ragionamento + personalità)
  </Accordion>

  <Accordion title="3. Hosting UE" icon="shield">
    **Serve hosting UE conforme al GDPR?**
    - **Azure OpenAI** è l'unico provider con hosting UE
    - Tutti i modelli GPT-4.1, GPT-4.1 Mini e GPT-5 disponibili
  </Accordion>
</AccordionGroup>

### Combinazioni Comuni di Modelli

Molti clienti utilizzano modelli diversi per agenti diversi:

```text wrap
Supporto Standard → GPT-4.1 Mini (miglior predefinito per la maggior parte degli agenti)
Routing Alto Volume → Groq Llama 3.1 8B (critico per velocità, compiti semplici)
Prenotazione Appuntamenti → GPT-4.1 Mini o Claude Haiku 4.5 (chiamata di strumenti affidabile)
Risoluzione Problemi Complessa → GPT-4.1 (quando serve più ragionamento)
Critico per Brand/Personalità → Claude Sonnet 4.5 (conversazioni più ricche)
```

---

## Testare le Prestazioni del Modello

### Test A/B dei Modelli

Per confrontare i modelli scientificamente:

1. **Duplica il tuo agente** nella dashboard
2. **Cambia solo il modello** su una versione
3. **Mantieni tutte le altre impostazioni identiche** (istruzioni, voce, azioni)
4. **Esegui scenari di test identici** su entrambi
5. **Confronta:**
   - Qualità e accuratezza delle risposte
   - Latenza e velocità
   - Naturalezza della conversazione
   - Affidabilità dell'attivazione delle azioni

### Criteri di Valutazione

Valuta ogni modello su:

| Criteri | Cosa Cercare |
|----------|-----------------|
| **Accuratezza** | Comprende correttamente le richieste? |
| **Aderenza alle Istruzioni** | Segue le regole del tuo prompt di sistema? |
| **Latenza** | Quanto velocemente risponde? |
| **Ritenzione del Contesto** | Ricorda la conversazione precedente? |
| **Tempistica Azioni** | Attiva le azioni nei momenti giusti? |
| **Gestione Errori** | Come gestisce richieste poco chiare? |

---

## Migliori Pratiche

<AccordionGroup>
  <Accordion title="Inizia con GPT-4.1 Mini" icon="rocket">
    Per la maggior parte degli agenti vocali, inizia con:
    - **Modello:** GPT-4.1 Mini
    - **Temperatura:** 0.0 (o 0.7 per più personalità)

    Cambia solo se i test mostrano che serve più intelligenza o velocità maggiore.
  </Accordion>

  <Accordion title="Non Spendere Troppo in Intelligenza" icon="scale-balanced">
    **Inizia in piccolo, aggiorna solo se necessario:**
    - La maggior parte dei casi d'uso funziona benissimo con GPT-4.1 Mini
    - Aggiorna a GPT-4.1 o Claude Sonnet 4.5 solo se Mini non può gestire la tua complessità
    - Usa Groq per routing/FAQ semplici dove la velocità conta più dell'intelligenza

    Abbina la capacità al requisito—non pagare per intelligenza che non ti serve.
  </Accordion>

  <Accordion title="Monitora le Prestazioni Reali" icon="chart-line">
    Usa le analitiche per tracciare:
    - Tempo medio di risposta
    - Tassi di successo delle azioni
    - Tassi di trasferimento (trasferimenti alti possono indicare problemi di ragionamento)
    - Punteggi di soddisfazione del cliente

    Cambia modelli se le metriche peggiorano.
  </Accordion>

  <Accordion title="Considera l'Implementazione Regionale" icon="globe">
    Se servi clienti globali:
    - Usa modelli ospitati in UE per chiamanti europei (GDPR)
    - Considera implementazioni Azure regionali per conformità enterprise
    - Considera la latenza dalla regione di hosting del modello ai clienti
  </Accordion>

  <Accordion title="Documenta i Cambiamenti del Modello" icon="file-pen">
    Quando cambi modelli in produzione:
    - Annota data e motivo nella descrizione dell'agente
    - Monitora le metriche per 24-48 ore dopo
    - Mantieni documentato l'ID del modello precedente per il rollback
    - Testa accuratamente prima di cambiare agenti ad alto volume
  </Accordion>
</AccordionGroup>

---

## Risoluzione Problemi del Modello

### Le Risposte dell'Agente Sono Troppo Verbose

**Soluzioni:**
- Aggiungi alle istruzioni: "Mantieni ogni risposta sotto 25 secondi"
- Usa temperatura 0.0 per risposte più focalizzate e concise
- Considera un modello più veloce che incoraggi la brevità

### L'Agente Fraintende le Richieste

**Soluzioni:**
- Passa a un modello di capacità superiore (GPT-4.1, Claude Sonnet 4.5)
- Migliora le istruzioni con esempi più specifici
- Aggiungi keyword boosting nelle impostazioni del trascrittore
- Rivedi prima l'accuratezza della trascrizione (potrebbe essere un problema STT, non LLM)

### L'Agente Non Segue le Istruzioni

**Soluzioni:**
- I modelli Claude tipicamente migliori nell'aderenza alle istruzioni
- Semplifica e chiarisci le istruzioni
- Usa elenchi puntati invece di paragrafi
- Aggiungi esempi espliciti di comportamento corretto
- Usa temperatura 0.0 per massima coerenza

### Latenza Alta / Risposte Lente

**Soluzioni:**
- Passa a un modello più veloce (Groq Llama 3.1 8B, Claude Haiku 4.5)
- Verifica se il problema è latenza del modello o di rete (testa con provider diversi)

### L'Agente Ripete le Stesse Frasi

**Soluzioni:**
- Aggiungi istruzione: "Varia la tua formulazione; evita espressioni ripetitive"
- Considera un modello diverso (alcuni hanno migliore diversità)
- Rivedi se le istruzioni causano inavvertitamente ripetizione

---

## Aggiornamenti e Versionamento dei Modelli

### Aggiornamenti dei Modelli dei Provider

I provider di modelli aggiornano regolarmente le loro offerte:
- **Aggiornamenti minori** spesso migliorano le prestazioni senza modifiche che interrompono
- **Cambiamenti di versione maggiori** (es. GPT-4 → GPT-5) possono richiedere test
- itellicoAI notifica i clienti prima degli aggiornamenti automatici di versione

### Controllo delle Versioni dei Modelli

Alcuni provider ti permettono di fissare versioni specifiche:
- **Latest:** Usa sempre la versione più recente (predefinito, consigliato)
- **Pinned:** Rimani su una versione specifica (usa se hai ottimizzato pesantemente per quel modello)

### Politica di Deprecazione

Quando i provider deprecano i modelli:
1. itellicoAI notifica i clienti interessati in anticipo
2. Percorso di migrazione consigliato fornito
3. Gli agenti vengono spostati automaticamente al modello successore se non viene intrapresa alcuna azione
4. Assistenza alla migrazione disponibile dal supporto

---

## Prossimi Passi

<CardGroup cols={2}>
  <Card title="Seleziona Voce" icon="microphone" href="/it/build/voice-speech/select-voice">
    Configura come suona il tuo agente con la selezione della voce
  </Card>
  <Card title="Configurazione Trascrittore" icon="ear" href="/it/build/voice-speech/transcriber">
    Scegli i modelli di trascrizione per ascoltare i clienti
  </Card>
  <Card title="Impostazioni Voce" icon="sliders" href="/it/build/voice-speech/voice-settings">
    Regola finemente velocità, tono e timbro per la tua voce
  </Card>
  <Card title="Testa il Tuo Agente" icon="vial" href="/it/test/web-simulator">
    Testa le prestazioni del modello con chiamate web
  </Card>
</CardGroup>
